{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Posts Classification Model Using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from tensorflow.keras.models import  Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home3/r05322021/Desktop/FB_hatecrime/Data/label/immigration_label.csv', encoding='utf-8', engine='python')\n",
    "df = df[(df.immigration_related.isin([0,1])) & (df.Mexican_related.isin([0,1]) & (df.Muslim_related.isin([0,1]))]\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(df, target_col):\n",
    "    df = df[df[target_col].notnull()]\n",
    "    df = df.fillna('')\n",
    "    for index in df.index:\n",
    "        for column in ['post_name', 'post_message', 'post_description']:\n",
    "            text = df.loc[index, column]\n",
    "            text = re.sub(r'http\\S+', '', str(text))\n",
    "            text = re.sub(r'\\@\\w+', '', str(text))\n",
    "            text = re.sub(r'#\\w+', '', str(text))\n",
    "            text = re.sub(r'\\[\\w+\\]', '', str(text))\n",
    "            text = re.sub(r'\\n', '', str(text))\n",
    "            text = re.sub(r'\\r', '', str(text))\n",
    "            df.loc[index, column] = text\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        #Cutting down the excess length\n",
    "        tokens = tokens[0:max_seq_length]\n",
    "        return [1]*len(tokens)\n",
    "    else :\n",
    "      return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    if len(tokens)>max_seq_length:\n",
    "      #Cutting down the excess length\n",
    "      tokens = tokens[:max_seq_length]\n",
    "      segments = []\n",
    "      current_segment_id = 0\n",
    "      for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "          current_segment_id = 1\n",
    "      return segments\n",
    "    else:\n",
    "      segments = []\n",
    "      current_segment_id = 0\n",
    "      for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "          current_segment_id = 1\n",
    "      return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):    \n",
    "    if len(tokens) > max_seq_length:\n",
    "      tokens = tokens[:max_seq_length]\n",
    "      token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      return token_ids\n",
    "    else:\n",
    "      token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "      return input_ids\n",
    "\n",
    "def preprocess_text(s, tokenizer, max_length):\n",
    "    stokens = tokenizer.tokenize(s)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "    input_ids = get_ids(stokens, tokenizer, max_length)\n",
    "    input_masks = get_masks(stokens, max_length)\n",
    "    input_segments = get_segments(stokens, max_length)\n",
    "    return input_ids, input_masks, input_segments\n",
    "\n",
    "def preprocess_whole(df, tokenizer, max_length, y_col):\n",
    "    \n",
    "    df = text_clean(df, y_col)\n",
    "    input_id, input_mask, input_segment = [], [], []\n",
    "    \n",
    "    for _, index in enumerate(tqdm(df.index.tolist(), total=len(df)), 1):\n",
    "        text = df['post_name'].loc[index] + ' ' + df['post_message'].loc[index] + ' ' + df['post_description'].loc[index]\n",
    "        ids, masks, segments = preprocess_text(text, tokenizer, max_length)\n",
    "        \n",
    "        input_id.append(ids)\n",
    "        input_mask.append(masks)\n",
    "        input_segment.append(segments)\n",
    "        \n",
    "    return [np.array(input_id, dtype=np.int32), \n",
    "            np.array(input_mask, dtype=np.int32), \n",
    "            np.array(input_segment, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(df, sample_num=100):\n",
    "    index = df.index.tolist()\n",
    "    test = random.sample(index, sample_num)\n",
    "    index = set(index) - set(test)\n",
    "    val = random.sample(index, sample_num)\n",
    "    train = set(index) - set(val)\n",
    "    \n",
    "    train = df.loc[train]\n",
    "    val = df.loc[val]\n",
    "    test = df.loc[test]\n",
    "    return train, val, test\n",
    "\n",
    "def training_form(df, sample_num, max_length, y_col):\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
    "    train, val, test = train_test_split(df, sample_num)\n",
    "    train_doc = train\n",
    "    \n",
    "    X_train = preprocess_whole(train, tokenizer, max_length, y_col)\n",
    "    X_val = preprocess_whole(val, tokenizer, max_length, y_col)\n",
    "    X_test = preprocess_whole(test, tokenizer, max_length, y_col)\n",
    "        \n",
    "    onehotencoder = OneHotEncoder()\n",
    "    y_train = onehotencoder.fit_transform(np.array(train[y_col]).reshape(-1, 1)).toarray()\n",
    "    y_val = onehotencoder.fit_transform(np.array(val[y_col]).reshape(-1, 1)).toarray()\n",
    "    y_test = onehotencoder.fit_transform(np.array(test[y_col]).reshape(-1, 1)).toarray()\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
    "MAX_SEQ_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4557/4557 [00:08<00:00, 540.23it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 539.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 612.09it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = training_form(df, 100, MAX_SEQ_LEN, 'immigration_negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                       name=\"input_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "input_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                    name=\"input_segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output, sequence_output = bert_layer([input_ids, input_mask, input_segment])\n",
    "bert_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Conv1D(filters=64, activation='relu', kernel_size=5, strides=1)(sequence_output)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"dense_output\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "      inputs=[input_ids, input_mask, input_segment], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4557 samples, validate on 100 samples\n",
      "Epoch 1/3\n",
      "4557/4557 - 2959s - loss: 0.6583 - accuracy: 0.8111 - val_loss: 1.1380 - val_accuracy: 0.7100\n",
      "Epoch 2/3\n",
      "4557/4557 - 2815s - loss: 0.4069 - accuracy: 0.9078 - val_loss: 1.0515 - val_accuracy: 0.6900\n",
      "Epoch 3/3\n",
      "4557/4557 - 2649s - loss: 0.3187 - accuracy: 0.9359 - val_loss: 1.2849 - val_accuracy: 0.7400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efab8e5c048>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=3,batch_size=128,verbose=2,validation_data=[X_val, Y_val], class_weight={0:1, 1:2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "def predict(original_df, threshold, model):\n",
    "    \n",
    "    prediction = model.predict(original_df, batch_size=128)\n",
    "    pred = [1 if d[1] >= threshold else 0 for d in prediction]\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Evaluation\n",
    "def Evaluation(original_df, original_label, model, threshold=0.5):\n",
    "    \n",
    "    Y = [0 if element[0] == 1 else 1 for element in original_label]\n",
    "    prediction = predict(original_df, threshold=threshold, model=model)\n",
    "    accuracy = metrics.accuracy_score(Y, prediction)\n",
    "\n",
    "    target_names = ['ACTIVE', 'CHURN']\n",
    "    report = classification_report(Y, prediction, target_names=target_names)\n",
    "\n",
    "    return [accuracy, report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 42s 418ms/sample - loss: 0.6422 - accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6422328320145607, 0.68]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ACTIVE       0.81      0.74      0.77        74\n",
      "       CHURN       0.41      0.50      0.45        26\n",
      "\n",
      "    accuracy                           0.68       100\n",
      "   macro avg       0.61      0.62      0.61       100\n",
      "weighted avg       0.70      0.68      0.69       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = Evaluation(X_test, Y_test, model)\n",
    "print(r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(r'/home3/r05322021/Desktop/FB_hatecrime/model/immigration_sentiment.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FB",
   "language": "python",
   "name": "fb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
